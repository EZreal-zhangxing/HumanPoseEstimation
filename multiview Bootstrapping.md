# Multiview Bootstrapping

## Training

关键点检测器$d(\cdot)$映射一块裁剪的输入图像$I \in \mathbb R^{w \times h \times 3}$到一个关键点位置$x_p \in \mathbb R^2$，每个关键点都带有一个对应的检测置信度$c_p$
$$
d(I) \rightarrow \{ (x_p,c_p) \ for\  p \in [1,\ldots,P]\}
$$
每个点$p$都代表着不同的关键点，见$Fig:4a$

![image-20220411143134829](.\image-20220411143134829.png)

假设目标在图像$I$中只有一个实例可见。通过裁剪出一大批只包含一个实例的数据集，让检测器在该数据集上训练。$(I^f,\{y_p^f\})$。$f$是指定的图像帧。$\{y_p^f\}$是图像$I^f$的所有有标记关键点集合。

初始化有$N_0$个训练对的训练集$T_0$，可以得到表达式如下：
$$
T_0 ；= \{(I^f,\{y_p^f\}) \ for \ f \in [1,\ldots,N_0]\}
$$
用这个初始化的训练集来训练检测器$d_0 := d_0 \leftarrow train(T_0)$，然后用这个检测器$d_0$和未标记多视角数据集。生成一个有标签的图像集合$T_1$，用这个数据集来提高检测器得到$d_1$
$$
d_1 \leftarrow train(T_0 \cup T_1)
$$
为了提高检测器$d_0$，我们需要一个额外的监督来确保$T_1$的信息并没有出现在$T_0$。这个监督就是：多视角几何验证；如果一个点至少在两个视图中被轻松识别，三角化的3D位置可以重新投影到其他视图的位置上。为检测失败的视图提供新的标注。

![image-20220411145312793](.\image-20220411145312793.png)



算法流程：

1. 对于每一帧，算法首先在每一个相机视图上独立运行并检测目标点$Fig:a,c$，然后进行三角化$Fig:b$
2. 更具分数对帧的集合进行排序，仅选择正确的三角化实例
3. 前N个最好的帧将三角点投射到所有视图上，然后被用于训练新的检测器$Fig:d$，给选中的N个帧的每一个帧生成接近$V$个训练图像

![image-20220411150404070](.\image-20220411150404070.png)

## 三角化

给定一个对象的$V$个视图在帧$f$中，在每个图像$I_v^f$运行检测器$d_i$ (在$T_i$上训练)，生成2D候选区的集合$D$
$$
D \leftarrow = \{ d_i(I_v^f) \ for \ v \in [1,\ldots,V] \}
$$
对于每个关键点$p$，我们都有$V$个检测点$(x_p^v,c_p^v)$，$x_p^v$是坐标，$c_p^v \in [0,1]$是置信度

在点$D$集上，使用RANSAC进行三角化，映射到3D空间。检测的置信度阈值为$\lambda$。RANSAC可接受的内点投影误差为4像素。综上可以得到我们要最小化投影误差方程如下：
$$
X_p^f = arg\min_X \sum_{v \in I_p^f} ||P_v(X) - x_p^v||_2^2
$$
$I_p^f$是内点集合。$X_p^f \in \mathbb R^3$是帧$f$中3D三角化的点$p$，$P_v(X) \in \mathbb R^2$记为3D点映射到2D视图$v$的方法。

每次计算时都对每个手指的四个点同时进行计算，**使用四个点的平均投影误差来确定RANSAC的内点。**

这样更稳健并且减少了三角化关键点的数量(需要四个点在图像中都可见)。



#### RANSAC，随机一致性采样

局内点：满足模型的点

局外点：不能适应模型的点

![v2-92f0ad1a9054d4bd19a759b7e3167bcc_r](.\v2-92f0ad1a9054d4bd19a759b7e3167bcc_r.jpg)



[RANSAC算法详解](https://zhuanlan.zhihu.com/p/62238520)

[计算机视觉基本原理——RANSAC](https://zhuanlan.zhihu.com/p/45532306)



## Scoring and Sorting

为了避免将错误标记的帧作为训练数据，所以我们只选择少量可靠的帧

为每$W$帧选择最优帧，最优帧的定义就是：内点中（帧）有最大的置信度和
$$
score(\{X_p^f\}) = \sum_{p \in [1,\ldots,P]} \sum_{v \in I_p^f} c_p^v
$$
对计算后的结果其进行降序排列

## Retraining

使用N个最优帧定义一个新的数据集来训练$i+1$个检测器。



## Detection Architecture

网络结构参考：CPMs

CPMs预测每个关键点的置信图。将关键点的位置表示成高斯分布。见$Fig:4b$

### keypoint detection

网络结构使用的是：VGG-19 Conv4_4层之前的预训练模型

带有两个附加卷积，生成128通道的特征$F$

生成一组$P$的置信度得分 $S^1 = \{S_1^1,\ldots,S_P^1\}$，其中$S_p^1 \in \mathbb R^{w’ \times h'}$ 是关键点$p$的置信度图

并且第一阶段之后的每个置信度的得分图$S^{t-1}$，都会Concat图像特征$F$后输入到后一个阶段$S^t$进行训练

使用6个连续的阶段，最后通过双三次差值还原到原始大小

损失：使用加权L2损失，如果该点没有标签那么权重为0

### Box Detection

对于开头提到的裁剪过的手附近的图像是如何生成的？

我们使用[openPose](./OpenPose.md)还有 Convolutional pose machines的估计模型。裁剪出$2.2B$大小的区域

紧包裹手的框面积为$B$

测试时$B = 0.7H$ 

$H$ 是头的面积

然后reSize到$368 \times 368$

